{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Assignment : Week 1 \n",
    "## Modeling simple RL problems by making their MDPs in Python\n",
    "\n",
    "We will create the MDPs for some of the example problems from Grokking textbook. For the simple environments, we can just hardcode the MDPs into a dictionary by exhaustively encoding the whole state space and the transition function. We will also go through a more complicated example where the state space is too large to be manually coded and we need to implement the transition function based on some state parameters.\n",
    "\n",
    "Later on, you will not need to implement the MDPs of common RL problems yourself, most of the work is already done by the OpenAI Gym library, which includes models for most of the famous RL envis.\n",
    "\n",
    "You can start this assignment during/after reading Grokking Ch-2."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Environment 0 - Bandit Walk\n",
    "\n",
    "Let us consider the BW environment on Page 39. \n",
    "\n",
    "State Space has 3 elements, states 0, 1 and 2.\n",
    "States 0 and 2 are terminal states and state 1 is the starting state.\n",
    "\n",
    "Action space has 2 elements, left and right.\n",
    "\n",
    "The environment is deterministic - transition probability of any action is 1.\n",
    "\n",
    "Only 1 (State, Action, State') tuple has positive reward, (1, Right, 2) gives the agent +1 reward."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "bw_mdp = {\n",
    "    0 : {\n",
    "        \"Right\" : [(1, 0, 0, True)],\n",
    "        \"Left\" : [(1, 0, 0, True)]\n",
    "    },\n",
    "    1 : {\n",
    "        \"Right\" : [(1, 2, 1, True)],\n",
    "        \"Left\" : [(1, 0, 0, True)]\n",
    "    },\n",
    "    2 : {\n",
    "        \"Right\" : [(1, 2, 0, True)],\n",
    "        \"Left\" : [(1, 2, 0, True)]\n",
    "    }\n",
    "}" 
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Environment 1 - Slippery Walk\n",
    "\n",
    "Now, we'll model the Slippery Walk MDP correctly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "swf_mdp = {\n",
    "    0 : {\n",
    "        \"Right\" : [(1, 0, 0, True)],\n",
    "        \"Left\" : [(1, 0, 0, True)]\n",
    "    },\n",
    "    1 : {\n",
    "        \"Right\" : [(1/2, 2, 0, False), (1/3, 1, 0, False), (1/6, 0, 0, True)],\n",
    "        \"Left\" : [(1, 0, 0, True)]\n",
    "    },\n",
    "    2 : {\n",
    "        \"Right\" : [(1/2, 3, 0, False), (1/3, 2, 0, False), (1/6, 1, 0, False)],\n",
    "        \"Left\" : [(1/2, 1, 0, False), (1/3, 2, 0, False), (1/6, 0, 0, True)]\n",
    "    },\n",
    "    3 : {\n",
    "        \"Right\" : [(1/2, 4, 0, False), (1/3, 3, 0, False), (1/6, 2, 0, False)],\n",
    "        \"Left\" : [(1/2, 2, 0, False), (1/3, 3, 0, False), (1/6, 1, 0, False)]\n",
    "    },\n",
    "    4 : {\n",
    "        \"Right\" : [(1/2, 5, 1, True), (1/3, 4, 0, False), (1/6, 3, 0, False)],\n",
    "        \"Left\" : [(1/2, 3, 0, False), (1/3, 4, 0, False), (1/6, 2, 0, False)]\n",
    "    },\n",
    "    5 : {\n",
    "        \"Right\" : [(1, 5, 0, True)],\n",
    "        \"Left\" : [(1, 5, 0, True)]\n",
    "    }\n",
    "}" 
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "fl_mdp = {5: {}, 7: {}, 11: {}, 12: {}, 15: {}}  # Terminal states\n",
    "for state in range(16):\n",
    "    if state in fl_mdp: continue  # Skip terminal states\n",
    "    fl_mdp[state] = {}\n",
    "    for action, move in [(\"Up\", -4), (\"Down\", 4), (\"Right\", 1), (\"Left\", -1)]:\n",
    "        next_state = state + move\n",
    "        if 0 <= next_state < 16 and (action in [\"Left\", \"Right\"] or state % 4 == next_state % 4):\n",
    "            reward = 1 if next_state == 15 else 0\n",
    "            done = next_state in fl_mdp\n",
    "            fl_mdp[state][action] = [(1, next_state, reward, done)]\n"
   ]
  }
 ]}

